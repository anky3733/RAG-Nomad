{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "393f9232-7198-4045-94b2-864e68dfcaa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e976744ffef2425db4173c0a8a0ffac8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/tmp/ipykernel_200859/4003885194.py:45: DeprecationWarning: Call to deprecated class method from_defaults. (ServiceContext is deprecated, please use `llama_index.settings.Settings` instead.) -- Deprecated since version 0.10.0.\n",
      "  service_context=ServiceContext.from_defaults(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, ServiceContext, PromptTemplate, set_global_service_context\n",
    "from llama_index.llms.huggingface import HuggingFaceLLM\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "import torch\n",
    "\n",
    "\n",
    "def build_llm_model(doc_folder: str) -> HuggingFaceLLM:\n",
    "\n",
    "    # Load documents\n",
    "    documents = SimpleDirectoryReader(doc_folder).load_data()\n",
    "\n",
    "    # Define system prompt and query wrapper prompt\n",
    "    system_prompt = \"\"\"You are a Q&A assistant. Your goal is to answer questions as accurately as possible based on the instructions and context provided.\"\"\"\n",
    "    query_wrapper_prompt = PromptTemplate(\"<|USER|>{query_str}<|ASSISTANT|>\")\n",
    "\n",
    "    # Create an LLM instance\n",
    "    llm = HuggingFaceLLM(\n",
    "        context_window=4096,\n",
    "        max_new_tokens=256,\n",
    "        generate_kwargs={\"temperature\": 0.0, \"do_sample\": False},\n",
    "        system_prompt=system_prompt,\n",
    "        query_wrapper_prompt=query_wrapper_prompt,\n",
    "        tokenizer_name=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "        model_name=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "        device_map=\"auto\",\n",
    "        # uncomment this if using CUDA to reduce memory usage\n",
    "        model_kwargs={\"torch_dtype\": torch.float16 , \"load_in_8bit\":True}\n",
    "    )\n",
    "\n",
    "    # Create an embedding model\n",
    "    embed_model = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-mpnet-base-v2\"\n",
    ")\n",
    "\n",
    "    service_context=ServiceContext.from_defaults(\n",
    "    chunk_size=1024,\n",
    "    llm=llm,\n",
    "    embed_model=embed_model\n",
    ")\n",
    "    # Create a vector store index\n",
    "    index=VectorStoreIndex.from_documents(documents,service_context=service_context)\n",
    "\n",
    "    return index\n",
    "\n",
    "index = build_llm_model(\"/data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "80b945b2-d8f0-4c53-a22f-299ad46d37ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " According to the provided context, NOMAD supports the transformation of materials-science data into knowledge and understanding by providing a platform for formalizing data acquisition, organizing and sharing data, homogenizing and normalizing data for analysis, and integrating with analysis tools. This enables researchers to put their data into machine and human comprehensible representations, making it FAIR (Findable, Accessible, Interoperable, and Reusable). Additionally, NOMAD provides an API and libraries for accessing and analyzing the NOMAD Archive data via state-of-the-art AI tools, allowing for the application of sophisticated AI algorithms to materials-science data. The NOMAD AI Toolkit also offers a user-friendly infrastructure for applying the latest AI developments and popular machine-learning methods to materials-science data, facilitating the deployment of AI-powered methodologies in the field. By providing a platform for data sharing, analysis, and publication, NOMAD supports the transformation of materials-science data into knowledge and understanding.\n"
     ]
    }
   ],
   "source": [
    "query_engine=index.as_query_engine()\n",
    "# response = service_context.generate_response(question)\n",
    "response=query_engine.query('How does NOMAD support the transformation of materials-science data into knowledge and understanding')\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9d7893-7533-451e-bbe6-101d0cf608e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
